{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Maybe try this - https://youtu.be/0VLAoVGf_74?si=zuJ8AL_wLbsbRdd5\n",
    "Compare MHA, MQA, MLA (DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply TransformerClassifier (Encoder Only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import *\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformer import TransformerClassifierMHA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 28000\n",
      "Validation size: 4000\n",
      "Test size: 8000\n"
     ]
    }
   ],
   "source": [
    "wassa_dataset = load_dataset(\"csv\", data_files=\"./dataset/text_emotion.csv\")\n",
    "wassa_dataset = wassa_dataset.rename_column('content', 'text')\n",
    "wassa_dataset_dict = create_train_validation_test(wassa_dataset['train'])\n",
    "\n",
    "embedding_matrix = np.load(EMBEDDING_PATH)\n",
    "\n",
    "with open(WORD2IDX_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentiment classes: 13\n",
      "Emotion classes: ['anger' 'boredom' 'empty' 'enthusiasm' 'fun' 'happiness' 'hate' 'love'\n",
      " 'neutral' 'relief' 'sadness' 'surprise' 'worry']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(wassa_dataset_dict['train']['sentiment'])\n",
    "val_labels = label_encoder.transform(wassa_dataset_dict['validation']['sentiment'])\n",
    "test_labels = label_encoder.transform(wassa_dataset_dict['test']['sentiment'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of sentiment classes: {num_classes}\")\n",
    "print(f\"Emotion classes: {label_encoder.classes_}\")\n",
    "\n",
    "wassa_labels_dict = {\n",
    "    'train': train_labels,\n",
    "    'validation': val_labels,\n",
    "    'test': test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoaders with 875 training batches, 125 validation batches, and 250 test batches.\n"
     ]
    }
   ],
   "source": [
    "wassa_dataloaders_dict = create_dataloaders(wassa_dataset_dict, wassa_labels_dict, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "TransformerClassifier_MHA = TransformerClassifierMHA(len(word2idx), 13, 100, 5, 3, 200, 100, 0.1)  \n",
    "# vocab_size, num_classes, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "TransformerClassifier_MHA.to(device)\n",
    "\n",
    "embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "TransformerClassifier_MHA.embedding.weight.data.copy_(embedding_matrix_tensor)\n",
    "TransformerClassifier_MHA.embedding.weight.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(TransformerClassifier_MHA.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
