{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Maybe try this - https://youtu.be/0VLAoVGf_74?si=zuJ8AL_wLbsbRdd5\n",
    "Compare MHA, MQA, MLA (DeepSeek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply TransformerClassifier (Encoder Only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import *\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformer import TransformerClassifierMHA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7fa4a666124435a7ef4718cd18fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4970\n",
      "Validation size: 711\n",
      "Test size: 1421\n"
     ]
    }
   ],
   "source": [
    "# CrowdFlower\n",
    "# dataset = load_dataset(\"csv\", data_files=\"./dataset/text_emotion.csv\")\n",
    "# dataset = crowd_dataset.rename_column('content', 'text')\n",
    "# dataset_dict = create_train_validation_test(crowd_dataset['train'])\n",
    "\n",
    "# Wassa\n",
    "dataset = load_dataset(\"csv\", data_files=\"./dataset/wassa_combined_data.csv\")\n",
    "dataset = dataset.rename_column('tweet', 'text')\n",
    "dataset_dict = create_train_validation_test(dataset['train'])\n",
    "\n",
    "embedding_matrix = np.load(EMBEDDING_PATH)\n",
    "\n",
    "with open(WORD2IDX_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentiment classes: 4\n",
      "Emotion classes: ['anger' 'fear' 'joy' 'sadness']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(dataset_dict['train']['sentiment'])\n",
    "val_labels = label_encoder.transform(dataset_dict['validation']['sentiment'])\n",
    "test_labels = label_encoder.transform(dataset_dict['test']['sentiment'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of sentiment classes: {num_classes}\")\n",
    "print(f\"Emotion classes: {label_encoder.classes_}\")\n",
    "\n",
    "crowd_labels_dict = {\n",
    "    'train': train_labels,\n",
    "    'validation': val_labels,\n",
    "    'test': test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoaders with 156 training batches, 23 validation batches, and 45 test batches.\n"
     ]
    }
   ],
   "source": [
    "dataloaders_dict = create_dataloaders(dataset_dict, crowd_labels_dict, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train the TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from common_utils import *\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformer import TransformerClassifierMHA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "TransformerClassifier_MHA = TransformerClassifierMHA(len(word2idx), 13, 100, 10, 6, 200, 100, 0.15)\n",
    "# vocab_size, num_classes, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "TransformerClassifier_MHA.to(device)\n",
    "\n",
    "embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "TransformerClassifier_MHA.embedding.weight.data.copy_(embedding_matrix_tensor)\n",
    "TransformerClassifier_MHA.embedding.weight.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(TransformerClassifier_MHA.parameters(), lr=0.001)\n",
    "\n",
    "total_steps = 100 * len(dataloaders_dict['train'])\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.001,\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 2.0954 Acc: 0.2908 F1: 0.1704\n",
      "validation Loss: 1.7011 Acc: 0.3116 F1: 0.1481\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 1.5498 Acc: 0.2926 F1: 0.2330\n",
      "validation Loss: 1.4207 Acc: 0.3116 F1: 0.1481\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 1.4345 Acc: 0.2825 F1: 0.2515\n",
      "validation Loss: 1.3855 Acc: 0.3116 F1: 0.1481\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 1.2922 Acc: 0.3943 F1: 0.3678\n",
      "validation Loss: 0.8700 Acc: 0.6471 F1: 0.6053\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.8483 Acc: 0.6631 F1: 0.6599\n",
      "validation Loss: 0.6553 Acc: 0.7434 F1: 0.7186\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.6446 Acc: 0.7627 F1: 0.7625\n",
      "validation Loss: 0.4780 Acc: 0.8260 F1: 0.8286\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.5210 Acc: 0.8231 F1: 0.8231\n",
      "validation Loss: 0.4742 Acc: 0.8461 F1: 0.8455\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.4224 Acc: 0.8539 F1: 0.8538\n",
      "validation Loss: 0.3383 Acc: 0.8698 F1: 0.8703\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.3736 Acc: 0.8725 F1: 0.8726\n",
      "validation Loss: 0.3638 Acc: 0.8648 F1: 0.8658\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.3185 Acc: 0.8925 F1: 0.8926\n",
      "validation Loss: 0.3769 Acc: 0.8824 F1: 0.8826\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.2624 Acc: 0.9108 F1: 0.9109\n",
      "validation Loss: 0.3706 Acc: 0.8811 F1: 0.8815\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.2392 Acc: 0.9163 F1: 0.9165\n",
      "validation Loss: 0.3192 Acc: 0.8861 F1: 0.8865\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.1956 Acc: 0.9324 F1: 0.9325\n",
      "validation Loss: 0.3512 Acc: 0.8886 F1: 0.8892\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.9363 F1: 0.9364\n",
      "validation Loss: 0.3242 Acc: 0.8886 F1: 0.8890\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.1711 Acc: 0.9354 F1: 0.9355\n",
      "validation Loss: 0.3473 Acc: 0.8974 F1: 0.8974\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.1495 Acc: 0.9467 F1: 0.9468\n",
      "validation Loss: 0.3273 Acc: 0.8836 F1: 0.8835\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.1381 Acc: 0.9481 F1: 0.9482\n",
      "validation Loss: 0.3651 Acc: 0.8849 F1: 0.8849\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.9548 F1: 0.9548\n",
      "validation Loss: 0.3567 Acc: 0.8874 F1: 0.8881\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.1325 Acc: 0.9489 F1: 0.9489\n",
      "validation Loss: 0.4348 Acc: 0.8836 F1: 0.8838\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.1286 Acc: 0.9521 F1: 0.9522\n",
      "validation Loss: 0.3291 Acc: 0.8773 F1: 0.8776\n",
      "No improvement for 5 epochs\n",
      "Early stopping triggered after 20 epochs\n",
      "Training complete in 1m 33s\n",
      "Best val F1: 0.8974\n",
      "Test Accuracy: 0.8875\n",
      "Test F1 Score: 0.8880\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_val_f1 = 0.0\n",
    "\n",
    "    patience = 5\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1:.4f}')\n",
    "\n",
    "            if phase == 'validation':\n",
    "                if epoch_f1 > best_val_f1:\n",
    "                    best_val_f1 = epoch_f1\n",
    "                    best_model_wts = model.state_dict().copy()\n",
    "                    no_improve_epochs = 0\n",
    "\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'val_f1': epoch_f1,\n",
    "                    }, 'best_transformer_MHA_model.pt')\n",
    "                else:\n",
    "                    no_improve_epochs += 1\n",
    "                    print(f'No improvement for {no_improve_epochs} epochs')\n",
    "\n",
    "                if no_improve_epochs >= patience:\n",
    "                    print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                    time_elapsed = time.time() - since\n",
    "                    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "                    print(f'Best val F1: {best_val_f1:.4f}')\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val F1: {best_val_f1:.4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "trained_model = train_model(TransformerClassifier_MHA, dataloaders_dict, criterion, optimizer, scheduler, num_epochs=100)\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'Test F1 Score: {test_f1:.4f}')\n",
    "\n",
    "    return test_acc, test_f1\n",
    "\n",
    "test_acc, test_f1 = evaluate_model(trained_model, dataloaders_dict['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
