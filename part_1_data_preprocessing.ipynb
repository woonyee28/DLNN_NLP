{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data preprocessing\n",
    "In this part, we are going to perform a `step by step actions to convert our text to embeddings`. Firstly, we feed our dataset into the `tokenizer` function from common_utils.py. This step will conver a sequence of text into separable token. For example: text = 'I love this flavor!' will become ['I', 'love', 'this', 'flavor', '!']. After having the words splitted up, we can apply the current state-of-the-art context aware `word embeddings` - BERT, to convert our one-hot vector to context-aware embeddings. For example, the first splitted word 'I' will initially have a one hot vector with the size of the vector equals to the size of the vocabulary. 'I' == [0,0,0,0,...,0,1,0.....,0,0] == Token IDs (integers). Applying BERT on it will make the token ID into context aware embeddings. 'I' == [0.1,0.34,0.56,....,0.9,0.5]. After this step, we are ready to use it as the `input for our model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 59121\n",
      "Vocabulary saved to ./result/vocab.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wassa_dataset = load_dataset(\"csv\", data_files=\"./dataset/text_emotion.csv\")\n",
    "wassa_dataset = wassa_dataset.rename_column('content', 'text')\n",
    "\n",
    "wassa_vocab = tokenize(wassa_dataset['train'], save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GloVe words loaded: 400000\n",
      "Word to Embeddings saved to ./result/embedding.json\n"
     ]
    }
   ],
   "source": [
    "word_to_embeddings_dict = create_embedding_matrix(wassa_vocab, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59121"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_embeddings_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
